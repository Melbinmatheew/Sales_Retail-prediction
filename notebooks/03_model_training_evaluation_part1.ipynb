{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feature-Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/feature_engineered_sales_data.csv', index_col='Order Date', parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable (y)\n",
    "y = df['Sales']\n",
    "\n",
    "# Select relevant features (X)\n",
    "# Dropping some categorical and ID columns not used directly in this model type, and also the target variable itself\n",
    "potential_features = [\n",
    "    'Year', 'Month', 'Day', 'DayOfWeek', 'Quarter', 'WeekOfYear', \n",
    "    'Sales_Lag_1', 'Sales_Lag_7', 'Sales_Lag_30',\n",
    "    'Sales_Rolling_Mean_7D', 'Sales_Rolling_Mean_30D',\n",
    "    'Quantity', 'Discount', 'Profit' # Adding other numerical features that might be relevant\n",
    "]\n",
    "\n",
    "# Keep only features that are actually in the DataFrame columns\n",
    "X = df[[col for col in potential_features if col in df.columns]]\n",
    "\n",
    "# Display selected features\n",
    "print(\"Selected Features:\")\n",
    "print(X.columns)\n",
    "\n",
    "# Handle NaN values created by lag/rolling features by dropping rows with any NaNs\n",
    "# This also handles NaNs in y if they exist, but 'Sales' should not have NaNs from previous step\n",
    "X_y = X.copy()\n",
    "X_y['Sales'] = y\n",
    "X_y_cleaned = X_y.dropna()\n",
    "\n",
    "X = X_y_cleaned.drop(columns=['Sales'])\n",
    "y = X_y_cleaned['Sales']\n",
    "\n",
    "print(f\"\\nShape of X after dropping NaNs: {X.shape}\")\n",
    "print(f\"Shape of y after dropping NaNs: {y.shape}\")\n",
    "\n",
    "# Ensure data is sorted by 'Order Date' (already done by index, but good to confirm)\n",
    "X = X.sort_index()\n",
    "y = y.sort_index()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split (Initial - For context, Walk-Forward is Primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a split point, e.g., last year for testing\n",
    "unique_years = sorted(X.index.year.unique())\n",
    "if len(unique_years) > 1:\n",
    "    test_year_start = unique_years[-1]\n",
    "    split_date = pd.Timestamp(f'{test_year_start-1}-12-31') # End of the year before the last year\n",
    "else: # Handle case with only one year of data\n",
    "    split_date = X.index.min() + (X.index.max() - X.index.min()) * 0.8 # 80-20 split if only one year\n",
    "\n",
    "X_train_initial = X[X.index <= split_date]\n",
    "y_train_initial = y[y.index <= split_date]\n",
    "X_test_initial = X[X.index > split_date]\n",
    "y_test_initial = y[y.index > split_date]\n",
    "\n",
    "print(f\"Initial Train set size: {X_train_initial.shape[0]} ({X_train_initial.index.min()} to {X_train_initial.index.max()})\")\n",
    "print(f\"Initial Test set size: {X_test_initial.shape[0]} ({X_test_initial.index.min()} to {X_test_initial.index.max()})\")\n",
    "\n",
    "# This initial split is mostly for context. We'll use TimeSeriesSplit for actual model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Walk-Forward Validation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk-forward validation (or rolling forecast origin) is a method for evaluating time series models. Unlike standard k-fold cross-validation, it respects the temporal order of observations.\n",
    "\n",
    "Here's how it works:\n",
    "1.  A small portion of the data is used for initial training (e.g., the first `n` observations).\n",
    "2.  The model makes predictions for the next `m` observations (the test set for this fold).\n",
    "3.  The actual values for these `m` observations are then added to the training set.\n",
    "4.  The model is retrained (or updated) with this expanded training set.\n",
    "5.  Steps 2-4 are repeated, moving the forecast origin forward by `m` observations each time, until all available data has been used for testing.\n",
    "\n",
    "Scikit-learn's `TimeSeriesSplit` provides a convenient way to generate the indices for these splits. For each split, it provides a training set that is always prior to the test set. The training set size can either grow with each split, or a fixed-size sliding window can be used.\n",
    "\n",
    "For this notebook, we will use `TimeSeriesSplit` to iterate through several train/test splits. Models will be trained on the training part of each split and evaluated on the corresponding test part. The performance metrics will then be averaged across all splits or calculated on the combined set of out-of-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5 # Number of splits for TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# To store results for each model\n",
    "results_summary = pd.DataFrame(columns=['Model', 'MAE', 'MSE', 'RMSE', 'MAPE'])\n",
    "\n",
    "# Dictionary to store all out-of-sample predictions and actuals for each model\n",
    "model_predictions = {}\n",
    "all_actuals = {} # Stores y_test for each model, should be mostly the same for regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # Calculate MAPE, handling potential zero values in y_true to avoid division by zero\n",
    "    y_true_mape, y_pred_mape = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true_mape != 0\n",
    "    mape = np.mean(np.abs((y_true_mape[mask] - y_pred_mape[mask]) / y_true_mape[mask])) * 100\n",
    "    \n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.4f}%\")\n",
    "    return {'Model': model_name, 'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Forecast (Sales[t] = Sales[t-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_naive_all = []\n",
    "y_pred_naive_all = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    if not y_train_fold.empty: # Need at least one value in train to make a prediction\n",
    "        # Naive forecast: last value of training set is prediction for all test set\n",
    "        # More accurately, for each step in test, prediction is previous actual value\n",
    "        # For walk-forward, this means y_pred[t] = y_actual[t-1]\n",
    "        \n",
    "        # Get the actual values that would serve as naive predictions for the test fold\n",
    "        # This means y_test_fold's predictions are based on values immediately preceding it.\n",
    "        # The first prediction for y_test_fold[0] is y.iloc[test_index[0]-1]\n",
    "        current_preds_naive = []\n",
    "        for i in range(len(y_test_fold)):\n",
    "            actual_idx_for_pred = test_index[i] - 1\n",
    "            if actual_idx_for_pred >= 0: # Ensure we don't go before the start of the series\n",
    "                 current_preds_naive.append(y.iloc[actual_idx_for_pred])\n",
    "            else: # Should not happen if test_index[0] > 0\n",
    "                 current_preds_naive.append(0) # Or some other fill value\n",
    "        \n",
    "        y_true_naive_all.extend(y_test_fold.values)\n",
    "        y_pred_naive_all.extend(current_preds_naive)\n",
    "\n",
    "if y_true_naive_all:\n",
    "    naive_metrics = calculate_metrics(y_true_naive_all, y_pred_naive_all, 'Naive Forecast')\n",
    "    results_summary = pd.concat([results_summary, pd.DataFrame([naive_metrics])], ignore_index=True)\n",
    "    model_predictions['Naive Forecast'] = np.array(y_pred_naive_all)\n",
    "    all_actuals['Naive Forecast'] = np.array(y_true_naive_all)\n",
    "else:\n",
    "    print(\"Could not evaluate Naive Forecast due to empty test sets or insufficient data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Naive Forecast (Sales[t] = Sales[t-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_seasonal_naive_all = []\n",
    "y_pred_seasonal_naive_all = []\n",
    "seasonal_period = 7 # Daily data, so 7 for weekly seasonality\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    current_preds_seasonal_naive = []\n",
    "    for i in range(len(y_test_fold)):\n",
    "        actual_idx_for_pred = test_index[i] - seasonal_period\n",
    "        if actual_idx_for_pred >= 0: # Ensure we don't go before the start of the series\n",
    "            current_preds_seasonal_naive.append(y.iloc[actual_idx_for_pred])\n",
    "        else: # If t-7 is not available (e.g. first week of data)\n",
    "            # Fallback: use naive (t-1) or 0 or mean of available history\n",
    "            # Using t-1 if available, else 0\n",
    "            fallback_idx = test_index[i] - 1\n",
    "            if fallback_idx >=0:\n",
    "                current_preds_seasonal_naive.append(y.iloc[fallback_idx]) \n",
    "            else:\n",
    "                current_preds_seasonal_naive.append(0) # Or y_train_fold.mean() if y_train_fold not empty\n",
    "                \n",
    "    y_true_seasonal_naive_all.extend(y_test_fold.values)\n",
    "    y_pred_seasonal_naive_all.extend(current_preds_seasonal_naive)\n",
    "\n",
    "if y_true_seasonal_naive_all:\n",
    "    seasonal_naive_metrics = calculate_metrics(y_true_seasonal_naive_all, y_pred_seasonal_naive_all, 'Seasonal Naive Forecast (7-day)')\n",
    "    results_summary = pd.concat([results_summary, pd.DataFrame([seasonal_naive_metrics])], ignore_index=True)\n",
    "    model_predictions['Seasonal Naive Forecast (7-day)'] = np.array(y_pred_seasonal_naive_all)\n",
    "    all_actuals['Seasonal Naive Forecast (7-day)'] = np.array(y_true_seasonal_naive_all)\n",
    "else:\n",
    "    print(\"Could not evaluate Seasonal Naive Forecast due to empty test sets or insufficient data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression Models (with Walk-Forward Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest Regressor': RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining and evaluating {model_name}...\")\n",
    "    y_true_model_all = []\n",
    "    y_pred_model_all = []\n",
    "    \n",
    "    fold_num = 1\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        print(f\"  Fold {fold_num}: Train size={len(X_train_fold)}, Test size={len(X_test_fold)}\")\n",
    "        \n",
    "        if X_train_fold.empty or X_test_fold.empty:\n",
    "            print(f\"    Skipping Fold {fold_num} due to empty train/test set.\")\n",
    "            fold_num += 1\n",
    "            continue\n",
    "            \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        predictions = model.predict(X_test_fold)\n",
    "        \n",
    "        y_true_model_all.extend(y_test_fold.values)\n",
    "        y_pred_model_all.extend(predictions)\n",
    "        fold_num += 1\n",
    "        \n",
    "    if y_true_model_all:\n",
    "        model_metrics = calculate_metrics(y_true_model_all, y_pred_model_all, model_name)\n",
    "        results_summary = pd.concat([results_summary, pd.DataFrame([model_metrics])], ignore_index=True)\n",
    "        model_predictions[model_name] = np.array(y_pred_model_all)\n",
    "        all_actuals[model_name] = np.array(y_true_model_all) # Storing actuals for consistency\n",
    "    else:\n",
    "        print(f\"Could not evaluate {model_name} due to no predictions being made (e.g. all folds skipped).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Overall Model Performance Summary ---\")\n",
    "results_summary = results_summary.sort_values(by='RMSE').reset_index(drop=True)\n",
    "print(results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Initial Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above summarizes the performance of the baseline and regression models using walk-forward validation. Key metrics include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "**Baseline Performance:**\n",
    "-   The **Naive Forecast** (predicting current sales as the previous day's sales) provides a basic benchmark. Its performance indicates the level of day-to-day volatility.\n",
    "-   The **Seasonal Naive Forecast (7-day)** (predicting current sales as sales from 7 days ago) often performs better than the simple Naive Forecast if weekly seasonality is present, which is common in retail sales.\n",
    "\n",
    "**Regression Model Performance:**\n",
    "-   **Linear Regression:** This model provides a linear relationship between the features (time-based, lags, rolling means, etc.) and sales. Its performance will indicate if a simple linear model can capture the underlying trends and seasonality to some extent.\n",
    "-   **Decision Tree Regressor:** This model can capture non-linear relationships. However, a single decision tree can be prone to overfitting, especially if not pruned. Its performance will be interesting to compare against linear regression and random forest.\n",
    "-   **Random Forest Regressor:** As an ensemble of decision trees, Random Forest typically offers better generalization and is less prone to overfitting than a single decision tree. It can capture complex non-linear patterns and interactions between features. We often expect Random Forest to outperform the other models if the relationships are indeed complex and non-linear.\n",
    "\n",
    "**Comparison and Next Steps:**\n",
    "-   Comparing all models, we look for the one with the lowest error metrics (MAE, MSE, RMSE, MAPE). MAPE is particularly useful for understanding the error in percentage terms, making it comparable across different scales.\n",
    "-   If regression models significantly outperform the baselines, it suggests that the engineered features (lags, rolling means, time components) are informative.\n",
    "-   The Random Forest Regressor is often a strong contender. If it performs best, further hyperparameter tuning for it could yield even better results.\n",
    "-   If the performance is not satisfactory, we might need to consider:\n",
    "    *   More sophisticated feature engineering (e.g., interaction terms, more complex lag structures, holiday indicators).\n",
    "    *   Advanced time series models (e.g., ARIMA, SARIMA, Prophet, or deep learning models like LSTMs) which are specifically designed for time series data and can capture more complex temporal dependencies and seasonality patterns. These will be explored in Part 2.\n",
    "    *   Careful examination of residuals from the best model to identify any remaining patterns or areas for improvement.\n",
    "\n",
    "*(The actual numbers in the summary table will drive the specific conclusions once the notebook is executed.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions vs Actuals (Example for one model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting for Random Forest Regressor if it was run\n",
    "model_to_plot = 'Random Forest Regressor'\n",
    "if model_to_plot in model_predictions and model_to_plot in all_actuals:\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    # Get the corresponding actuals and predictions\n",
    "    actuals = all_actuals[model_to_plot]\n",
    "    predictions = model_predictions[model_to_plot]\n",
    "    \n",
    "    # We need a time index for these. The `all_actuals` were collected sequentially from test folds.\n",
    "    # Reconstruct the time index for the plotted data.\n",
    "    # This assumes `y` is sorted and `tscv.split(X)` produces contiguous test sets over time.\n",
    "    \n",
    "    # Find the starting index of the first test fold\n",
    "    first_test_fold_start_idx = -1\n",
    "    for _, test_index in tscv.split(X):\n",
    "        if first_test_fold_start_idx == -1:\n",
    "            first_test_fold_start_idx = test_index[0]\n",
    "        # Ensure we are using the correct slice of y for index\n",
    "        # The total length of predictions should match the total length of test data elements from y\n",
    "        # The y_true_model_all (which is all_actuals[model_name]) are collected in order.\n",
    "        # So, we need to find the date index of y that corresponds to the start of these collections.\n",
    "        \n",
    "    # This logic for test_idx_for_plot is simplified and might need adjustment\n",
    "    # It assumes y_true_model_all are from the latter part of the original y series\n",
    "    num_predictions = len(actuals)\n",
    "    test_idx_for_plot = y.index[-num_predictions:]\n",
    "    \n",
    "    if len(test_idx_for_plot) == len(actuals):\n",
    "        plt.plot(test_idx_for_plot, actuals, label='Actual Sales', alpha=0.7)\n",
    "        plt.plot(test_idx_for_plot, predictions, label=f'{model_to_plot} Predictions', linestyle='--')\n",
    "        plt.title(f'{model_to_plot}: Actual vs. Predicted Sales (Walk-Forward Validation)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sales')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Could not plot for {model_to_plot}: Mismatch in length of time index and predictions/actuals.\")\n",
    "        print(f\"  Length of time index: {len(test_idx_for_plot)}\")\n",
    "        print(f\"  Length of actuals: {len(actuals)}\")\n",
    "else:\n",
    "    print(f\"Model '{model_to_plot}' not found in results for plotting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

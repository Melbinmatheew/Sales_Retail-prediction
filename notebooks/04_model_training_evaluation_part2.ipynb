{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation (Part 2: Advanced Time Series Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Attempt to import pmdarima\n",
    "try:\n",
    "    import pmdarima as pm\n",
    "    pmdarima_available = True\n",
    "    print(\"pmdarima imported successfully.\")\n",
    "except ImportError:\n",
    "    pmdarima_available = False\n",
    "    print(\"pmdarima not found. Will use manual ACF/PACF for SARIMA parameter selection.\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Helper function to calculate metrics (consistent with Part 1)\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    y_true_mape, y_pred_mape = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true_mape != 0\n",
    "    mape = np.mean(np.abs((y_true_mape[mask] - y_pred_mape[mask]) / y_true_mape[mask])) * 100 if np.sum(mask) > 0 else np.nan\n",
    "    \n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.4f}%\")\n",
    "    return {'Model': model_name, 'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feature-Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('../data/feature_engineered_sales_data.csv', index_col='Order Date', parse_dates=True)\n",
    "# For SARIMA and Holt-Winters, we typically use univariate series, but exog can be used.\n",
    "# Here, we focus on univariate 'Sales' for simplicity in these models first.\n",
    "# We will use daily sales. If there are multiple transactions per day, sum them up.\n",
    "daily_sales = df_full['Sales'].resample('D').sum().fillna(0) # Resample to daily, sum sales, fill missing days with 0\n",
    "print(f\"Daily sales data from {daily_sales.index.min()} to {daily_sales.index.max()} with {len(daily_sales)} observations.\")\n",
    "daily_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation & Stationarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series, series_name=''):\n",
    "    print(f'Augmented Dickey-Fuller Test for {series_name}:')\n",
    "    result = adfuller(series.dropna()) # dropna for safety\n",
    "    print(f'  ADF Statistic: {result[0]:.4f}')\n",
    "    print(f'  p-value: {result[1]:.4f}')\n",
    "    print('  Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'    {key}: {value:.4f}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(f'  Result: Series \\\"{series_name}\\\" is likely stationary (p-value <= 0.05).')\n",
    "    else:\n",
    "        print(f'  Result: Series \\\"{series_name}\\\" is likely non-stationary (p-value > 0.05).')\n",
    "    return result[1]\n",
    "\n",
    "# Test original series\n",
    "p_value_orig = adf_test(daily_sales, 'Original Daily Sales')\n",
    "\n",
    "daily_sales_diff = daily_sales\n",
    "d = 0 # Order of non-seasonal differencing\n",
    "if p_value_orig > 0.05:\n",
    "    print(\"\\nOriginal series is non-stationary. Applying first differencing.\")\n",
    "    daily_sales_diff = daily_sales.diff(1).dropna()\n",
    "    d = 1\n",
    "    p_value_diff1 = adf_test(daily_sales_diff, 'First Differenced Daily Sales')\n",
    "    if p_value_diff1 > 0.05:\n",
    "        print(\"\\nFirst differenced series is still non-stationary. Applying second differencing.\")\n",
    "        daily_sales_diff = daily_sales.diff(1).diff(1).dropna()\n",
    "        d = 2\n",
    "        adf_test(daily_sales_diff, 'Second Differenced Daily Sales')\n",
    "else:\n",
    "    print(\"\\nOriginal series is stationary. No differencing needed (d=0).\")\n",
    "\n",
    "print(f\"\\nOrder of non-seasonal differencing to be used (d): {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Walk-Forward Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# To store results for each model\n",
    "results_summary_part2 = pd.DataFrame(columns=['Model', 'MAE', 'MSE', 'RMSE', 'MAPE'])\n",
    "\n",
    "# Dictionary to store all out-of-sample predictions and actuals\n",
    "model_predictions_part2 = {}\n",
    "all_actuals_part2 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_sarima_all = []\n",
    "y_pred_sarima_all = []\n",
    "seasonal_period = 7 # Assuming weekly seasonality for daily data\n",
    "\n",
    "print(\"\\n--- SARIMA Model Training (Walk-Forward) ---\")\n",
    "fold_num = 1\n",
    "for train_index, test_index in tscv.split(daily_sales):\n",
    "    train_data = daily_sales.iloc[train_index]\n",
    "    test_data = daily_sales.iloc[test_index]\n",
    "    \n",
    "    print(f\"  Fold {fold_num}: Train size={len(train_data)}, Test size={len(test_data)}\")\n",
    "    if train_data.empty or test_data.empty:\n",
    "        print(f\"    Skipping Fold {fold_num} due to empty train/test set.\")\n",
    "        fold_num += 1\n",
    "        continue\n",
    "\n",
    "    best_order = (1, d, 1) # Default non-seasonal order, d from ADF test\n",
    "    best_seasonal_order = (1, 1, 1, seasonal_period) # Default seasonal order, D=1 for seasonal differencing\n",
    "\n",
    "    if pmdarima_available:\n",
    "        try:\n",
    "            print(\"    Using pmdarima.auto_arima for parameter selection...\")\n",
    "            auto_arima_model = pm.auto_arima(train_data, \n",
    "                                             start_p=1, start_q=1, \n",
    "                                             max_p=3, max_q=3, \n",
    "                                             start_P=0, start_Q=0,\n",
    "                                             max_P=2, max_Q=2,\n",
    "                                             m=seasonal_period, \n",
    "                                             seasonal=True, \n",
    "                                             d=d, # Use pre-determined d from stationarity tests\n",
    "                                             D=1, # Let auto_arima find D, or set if known\n",
    "                                             trace=False, # Set to True for detailed logs\n",
    "                                             error_action='ignore', \n",
    "                                             suppress_warnings=True, \n",
    "                                             stepwise=True)\n",
    "            best_order = auto_arima_model.order\n",
    "            best_seasonal_order = auto_arima_model.seasonal_order\n",
    "            print(f\"    AutoARIMA chosen order: {best_order}, seasonal_order: {best_seasonal_order}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    auto_arima failed: {e}. Using default SARIMA parameters.\")\n",
    "    else: # Manual ACF/PACF analysis (simplified here)\n",
    "        print(\"    pmdarima not available. Using default SARIMA parameters after ACF/PACF (visual inspection needed).\")\n",
    "        # In a real scenario, you would plot ACF/PACF of train_data (differenced if needed)\n",
    "        # fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        # plot_acf(train_data.diff(d).dropna() if d > 0 else train_data, lags=40, ax=ax[0])\n",
    "        # plot_pacf(train_data.diff(d).dropna() if d > 0 else train_data, lags=40, ax=ax[1])\n",
    "        # plt.show()\n",
    "        # For this automated script, we stick to defaults or simple choices.\n",
    "        # d is already determined. Let's assume D=1 for seasonal differencing.\n",
    "        pass # Using the predefined defaults\n",
    "\n",
    "    try:\n",
    "        model_sarima = SARIMAX(train_data, \n",
    "                               order=best_order, \n",
    "                               seasonal_order=best_seasonal_order, \n",
    "                               enforce_stationarity=False, \n",
    "                               enforce_invertibility=False,\n",
    "                               initialization='approximate_diffuse') # Added initialization\n",
    "        # Fit model - increase maxiter if convergence issues, or try different solvers\n",
    "        sarima_fit = model_sarima.fit(disp=False, maxiter=200) \n",
    "        predictions = sarima_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
    "        \n",
    "        y_true_sarima_all.extend(test_data.values)\n",
    "        y_pred_sarima_all.extend(predictions.values)\n",
    "    except Exception as e:\n",
    "        print(f\"    SARIMA model fitting/prediction failed for Fold {fold_num}: {e}\")\n",
    "        # Fill with Naive forecast for this fold if SARIMA fails\n",
    "        naive_preds = [train_data.iloc[-1]] * len(test_data) if len(train_data) > 0 else [0] * len(test_data)\n",
    "        y_true_sarima_all.extend(test_data.values)\n",
    "        y_pred_sarima_all.extend(naive_preds)\n",
    "        \n",
    "    fold_num += 1\n",
    "\n",
    "if y_true_sarima_all:\n",
    "    sarima_metrics = calculate_metrics(y_true_sarima_all, y_pred_sarima_all, 'SARIMA')\n",
    "    results_summary_part2 = pd.concat([results_summary_part2, pd.DataFrame([sarima_metrics])], ignore_index=True)\n",
    "    model_predictions_part2['SARIMA'] = np.array(y_pred_sarima_all)\n",
    "    all_actuals_part2['SARIMA'] = np.array(y_true_sarima_all)\n",
    "else:\n",
    "    print(\"SARIMA model could not be evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exponential Smoothing (Holt-Winters) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_hw_all = []\n",
    "y_pred_hw_all = []\n",
    "seasonal_period_hw = 7 # Weekly seasonality\n",
    "\n",
    "print(\"\\n--- Exponential Smoothing (Holt-Winters) Model Training (Walk-Forward) ---\")\n",
    "fold_num = 1\n",
    "for train_index, test_index in tscv.split(daily_sales):\n",
    "    train_data = daily_sales.iloc[train_index]\n",
    "    test_data = daily_sales.iloc[test_index]\n",
    "    \n",
    "    print(f\"  Fold {fold_num}: Train size={len(train_data)}, Test size={len(test_data)}\")\n",
    "    if train_data.empty or test_data.empty or len(train_data) <= seasonal_period_hw*2: # HW needs enough data for seasonality\n",
    "        print(f\"    Skipping Fold {fold_num} due to insufficient data for Holt-Winters.\")\n",
    "        # Fill with Naive forecast for this fold if HW fails\n",
    "        naive_preds = [train_data.iloc[-1]] * len(test_data) if len(train_data) > 0 else [0] * len(test_data)\n",
    "        y_true_hw_all.extend(test_data.values) # Still need to add actuals\n",
    "        y_pred_hw_all.extend(naive_preds) # Add naive predictions\n",
    "        fold_num += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Try additive model first\n",
    "        model_hw = ExponentialSmoothing(train_data, \n",
    "                                        trend='add', \n",
    "                                        seasonal='add', \n",
    "                                        seasonal_periods=seasonal_period_hw,\n",
    "                                        initialization_method='estimated') # Changed init method\n",
    "        hw_fit = model_hw.fit()\n",
    "        predictions = hw_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
    "        \n",
    "        y_true_hw_all.extend(test_data.values)\n",
    "        y_pred_hw_all.extend(predictions.values)\n",
    "        \n",
    "    except Exception as e_add:\n",
    "        print(f\"    Additive Holt-Winters failed for Fold {fold_num}: {e_add}. Trying multiplicative.\")\n",
    "        try:\n",
    "            # Check for non-positive values if using multiplicative components\n",
    "            if (train_data <= 0).any():\n",
    "                print(\"    Skipping multiplicative Holt-Winters due to non-positive values in training data.\")\n",
    "                raise ValueError(\"Non-positive values in series, multiplicative trend/seasonality not suitable.\")\n",
    "\n",
    "            model_hw_mul = ExponentialSmoothing(train_data, \n",
    "                                              trend='mul', \n",
    "                                              seasonal='mul', \n",
    "                                              seasonal_periods=seasonal_period_hw,\n",
    "                                              initialization_method='estimated')\n",
    "            hw_fit_mul = model_hw_mul.fit()\n",
    "            predictions_mul = hw_fit_mul.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
    "            \n",
    "            y_true_hw_all.extend(test_data.values)\n",
    "            y_pred_hw_all.extend(predictions_mul.values)\n",
    "        except Exception as e_mul:\n",
    "            print(f\"    Multiplicative Holt-Winters also failed for Fold {fold_num}: {e_mul}. Using Naive for this fold.\")\n",
    "            naive_preds = [train_data.iloc[-1]] * len(test_data) if len(train_data) > 0 else [0] * len(test_data)\n",
    "            y_true_hw_all.extend(test_data.values)\n",
    "            y_pred_hw_all.extend(naive_preds)\n",
    "\n",
    "    fold_num += 1\n",
    "\n",
    "if y_true_hw_all:\n",
    "    hw_metrics = calculate_metrics(y_true_hw_all, y_pred_hw_all, 'Holt-Winters ES')\n",
    "    results_summary_part2 = pd.concat([results_summary_part2, pd.DataFrame([hw_metrics])], ignore_index=True)\n",
    "    model_predictions_part2['Holt-Winters ES'] = np.array(y_pred_hw_all)\n",
    "    all_actuals_part2['Holt-Winters ES'] = np.array(y_true_hw_all)\n",
    "else:\n",
    "    print(\"Holt-Winters model could not be evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Performance Summary (Part 2) ---\")\n",
    "results_summary_part2 = results_summary_part2.sort_values(by='RMSE').reset_index(drop=True)\n",
    "print(results_summary_part2)\n",
    "\n",
    "# Attempt to load results from Part 1 for a combined view\n",
    "try:\n",
    "    # This assumes the Part 1 notebook saved its results to a CSV, or we reconstruct it.\n",
    "    # For simplicity, let's assume Part 1's results_summary DataFrame is available or re-created if needed.\n",
    "    # As this is a new notebook execution, we'd typically load it from a file.\n",
    "    # If `03_model_training_evaluation_part1.ipynb` saved its `results_summary` to a CSV:\n",
    "    # results_summary_part1 = pd.read_csv('../reports/model_performance_part1.csv') \n",
    "    # For now, let's simulate it was loaded or passed (this part needs to be robust in a real pipeline)\n",
    "    print(\"\\nNote: Part 1 results would typically be loaded from a saved file.\")\n",
    "    # Placeholder for actual loading logic\n",
    "    # combined_results = pd.concat([results_summary_part1, results_summary_part2], ignore_index=True)\n",
    "    # combined_results = combined_results.sort_values(by='RMSE').reset_index(drop=True)\n",
    "    # print(\"\\n--- Combined Model Performance Summary (Part 1 & 2) ---\")\n",
    "    # print(combined_results)\n",
    "    print(\"Displaying only Part 2 results as Part 1 results loading is not implemented here.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\nPart 1 results file not found. Displaying only Part 2 results.\")\n",
    "    # combined_results = results_summary_part2.copy()\n",
    "\n",
    "# For the purpose of this task, we'll consider combined_results to be results_summary_part2 for identifying the best model from this notebook.\n",
    "# In a real scenario, you would load and merge results from Part 1.\n",
    "combined_results = results_summary_part2.copy() # Current best from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Time Series Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SARIMA Model:**\n",
    "-   SARIMA (Seasonal Autoregressive Integrated Moving Average) is a powerful model for time series data with seasonality.\n",
    "-   Parameter selection (p,d,q)(P,D,Q,s) is crucial. `pmdarima.auto_arima` automates this, which is very helpful. If not available, manual ACF/PACF analysis and grid search are needed, which can be time-consuming and require expertise.\n",
    "-   The 'd' parameter is determined from stationarity tests (ADF test).\n",
    "-   The seasonal period 's' (e.g., 7 for daily data with weekly seasonality) must be set appropriately.\n",
    "-   SARIMA can capture complex trend and seasonal patterns. Its performance depends heavily on correct parameter identification and the underlying data structure matching SARIMA assumptions.\n",
    "\n",
    "**Exponential Smoothing (Holt-Winters):**\n",
    "-   Holt-Winters is another effective method for data with trend and seasonality.\n",
    "-   It's generally simpler to configure than SARIMA (fewer parameters).\n",
    "-   We tested additive and potentially multiplicative versions for trend and seasonality. Multiplicative models can be useful if the seasonal variation or trend impact changes proportionally to the level of the series, but require positive data.\n",
    "-   `seasonal_periods` needs to be set according to the data's seasonality (e.g., 7 for weekly patterns in daily data).\n",
    "\n",
    "**Comparison to Regression Models (from Part 1):**\n",
    "-   **Strengths of SARIMA/Holt-Winters:** These models are explicitly designed for time series. They can capture autocorrelation and seasonality more directly than standard regression models unless regression models have very sophisticated lag/seasonal features.\n",
    "-   **Strengths of Regression Models:** Regression models (like Random Forest, Linear Regression from Part 1) can easily incorporate exogenous variables (e.g., promotions, holidays, economic indicators from `feature_engineered_sales_data.csv` like 'Discount', 'Profit', 'Quantity', or external data). While SARIMAX (the 'X' part) can also use exogenous variables, it's often simpler with regression frameworks.\n",
    "-   **Performance:**\n",
    "    -   If the time series has strong, regular seasonality and trend that SARIMA/Holt-Winters can model effectively, they might outperform regression models that rely on manually engineered time features.\n",
    "    -   If sales are heavily influenced by many external factors or complex non-linear relationships that are well-captured by the features in `X_train` (from Part 1), then models like Random Forest might perform better, especially if the time series structure is not perfectly regular.\n",
    "    -   The `Sales_Lag_` and `Sales_Rolling_Mean_` features in the regression models attempt to capture some of the autocorrelation that SARIMA models inherently handle.\n",
    "-   **Interpretability:** Linear Regression is highly interpretable. SARIMA and Holt-Winters parameters also have specific interpretations regarding trend and seasonality. Decision Trees and Random Forests can be less interpretable.\n",
    "-   **Computational Cost:** `auto_arima` can be computationally intensive. Holt-Winters is generally faster. Random Forest can also be slow to train with many trees/features.\n",
    "\n",
    "*(The actual numbers in the summary table will drive the specific conclusions about which model type is best for this dataset.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_results.empty:\n",
    "    best_model_row = combined_results.loc[combined_results['RMSE'].idxmin()] # Or MAE, or MAPE\n",
    "    best_model_name = best_model_row['Model']\n",
    "    print(f\"\\nBest performing model based on RMSE: {best_model_name} (RMSE: {best_model_row['RMSE']:.4f})\")\n",
    "\n",
    "    # Create models directory if it doesn't exist\n",
    "    if not os.path.exists('../models'):\n",
    "        os.makedirs('../models')\n",
    "        print(\"Created directory: ../models\")\n",
    "    \n",
    "    model_filename = f\"../models/best_sales_forecasting_model_{best_model_name.replace(' ', '_').lower()}.joblib\"\n",
    "\n",
    "    # Retrain the best model on the entire dataset before saving\n",
    "    # This logic needs to be adapted based on which model is best (from Part 1 or Part 2)\n",
    "    print(f\"Retraining {best_model_name} on the full daily_sales dataset...\")\n",
    "\n",
    "    final_model_to_save = None\n",
    "\n",
    "    if best_model_name == 'SARIMA':\n",
    "        # Retrain SARIMA on full daily_sales data\n",
    "        # Use parameters found earlier or run auto_arima on full data if pmdarima is available\n",
    "        final_sarima_order = (1, d, 1) # Example, should be from best fold or full auto_arima\n",
    "        final_seasonal_order = (1, 1, 1, seasonal_period) # Example\n",
    "        if pmdarima_available:\n",
    "            try:\n",
    "                auto_model_full = pm.auto_arima(daily_sales, start_p=1, start_q=1, max_p=3, max_q=3,\n",
    "                                                start_P=0, max_P=2, m=seasonal_period, seasonal=True,\n",
    "                                                d=d, D=1, trace=False, error_action='ignore',\n",
    "                                                suppress_warnings=True, stepwise=True)\n",
    "                final_sarima_order = auto_model_full.order\n",
    "                final_seasonal_order = auto_model_full.seasonal_order\n",
    "                print(f\"    AutoARIMA chosen order for full data: {final_sarima_order}, seasonal: {final_seasonal_order}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    auto_arima on full data failed: {e}. Using default parameters for final SARIMA.\")\n",
    "        \n",
    "        final_model_sarima = SARIMAX(daily_sales, order=final_sarima_order, seasonal_order=final_seasonal_order,\n",
    "                                   enforce_stationarity=False, enforce_invertibility=False, \n",
    "                                   initialization='approximate_diffuse')\n",
    "        final_model_to_save = final_model_sarima.fit(disp=False, maxiter=250)\n",
    "        print(f\"Final SARIMA model retrained.\")\n",
    "        joblib.dump(final_model_to_save, model_filename) # Statsmodels results objects can often be pickled\n",
    "        print(f\"Saved {best_model_name} to {model_filename}\")\n",
    "\n",
    "    elif best_model_name == 'Holt-Winters ES':\n",
    "        # Retrain Holt-Winters on full daily_sales data\n",
    "        try:\n",
    "            final_model_hw = ExponentialSmoothing(daily_sales, trend='add', seasonal='add', \n",
    "                                                seasonal_periods=seasonal_period_hw, \n",
    "                                                initialization_method='estimated').fit()\n",
    "            final_model_to_save = final_model_hw\n",
    "            print(f\"Final Holt-Winters model retrained.\")\n",
    "            joblib.dump(final_model_to_save, model_filename)\n",
    "            print(f\"Saved {best_model_name} to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retraining/saving Holt-Winters: {e}\")\n",
    "            if (daily_sales <= 0).any():\n",
    "                print(\"    Non-positive values in data might be an issue for multiplicative components if used.\")\n",
    "\n",
    "    # Add similar blocks if a model from Part 1 (e.g., RandomForestRegressor) was best.\n",
    "    # This would involve loading df_full, preparing X and y from it, and then training.\n",
    "    # Example for Random Forest (assuming it was best and named 'Random Forest Regressor'):\n",
    "    # elif best_model_name == 'Random Forest Regressor':\n",
    "    #     print(f\"Retraining Random Forest Regressor on the full dataset...\")\n",
    "    #     # Prepare X and y from df_full (similar to Part 1)\n",
    "    #     y_full = df_full['Sales']\n",
    "    #     potential_features_full = [\n",
    "    #         'Year', 'Month', 'Day', 'DayOfWeek', 'Quarter', 'WeekOfYear', \n",
    "    #         'Sales_Lag_1', 'Sales_Lag_7', 'Sales_Lag_30',\n",
    "    #         'Sales_Rolling_Mean_7D', 'Sales_Rolling_Mean_30D',\n",
    "    #         'Quantity', 'Discount', 'Profit'\n",
    "    #     ]\n",
    "    #     X_full = df_full[[col for col in potential_features_full if col in df_full.columns]]\n",
    "    #     X_y_full_combined = X_full.copy()\n",
    "    #     X_y_full_combined['Sales'] = y_full\n",
    "    #     X_y_full_cleaned = X_y_full_combined.dropna()\n",
    "    #     X_full_final = X_y_full_cleaned.drop(columns=['Sales'])\n",
    "    #     y_full_final = X_y_full_cleaned['Sales']\n",
    "    #     \n",
    "    #     from sklearn.ensemble import RandomForestRegressor\n",
    "    #     final_rf_model = RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1) # Use best params if tuned\n",
    "    #     final_rf_model.fit(X_full_final, y_full_final)\n",
    "    #     final_model_to_save = final_rf_model\n",
    "    #     joblib.dump(final_model_to_save, model_filename)\n",
    "    #     print(f\"Saved {best_model_name} to {model_filename}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Model '{best_model_name}' is from Part 1. Retraining and saving logic for it needs to be implemented here.\")\n",
    "        print(\"Please ensure the combined_results DataFrame accurately reflects all evaluated models from both parts.\")\n",
    "\n",
    "else:\n",
    "    print(\"No models were evaluated or results summary is empty. Cannot determine or save the best model.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions vs Actuals (Example for one model - e.g., SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_plot = 'SARIMA' # or 'Holt-Winters ES'\n",
    "if model_to_plot in model_predictions_part2 and model_to_plot in all_actuals_part2:\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    actuals = all_actuals_part2[model_to_plot]\n",
    "    predictions = model_predictions_part2[model_to_plot]\n",
    "    \n",
    "    # Determine the time index for the plotted data (collected from test folds)\n",
    "    num_predictions = len(actuals)\n",
    "    # The actuals/predictions are from the test sets of the walk-forward validation.\n",
    "    # We need to find the corresponding dates from `daily_sales.index`\n",
    "    \n",
    "    # This reconstruction assumes contiguous test splits from TimeSeriesSplit\n",
    "    # Find the total number of observations used across all test splits\n",
    "    total_test_obs = 0\n",
    "    test_indices_list = []\n",
    "    for _, test_idx_fold in tscv.split(daily_sales):\n",
    "        total_test_obs += len(test_idx_fold)\n",
    "        test_indices_list.extend(daily_sales.index[test_idx_fold])\n",
    "        \n",
    "    # Ensure the number of collected actuals matches the total test observations from splits\n",
    "    if len(actuals) == total_test_obs:\n",
    "        plot_index = pd.Index(test_indices_list).unique() # Get unique sorted dates\n",
    "        # This index might still be longer if folds overlapped, but usually TimeSeriesSplit gives contiguous blocks\n",
    "        # For simplicity, we assume the collected actuals align with the latter part of the series\n",
    "        # A more robust way is to store indices along with predictions in the loop.\n",
    "        plot_index_final = daily_sales.index[-len(actuals):] \n",
    "\n",
    "        plt.plot(plot_index_final, actuals, label='Actual Sales', alpha=0.7)\n",
    "        plt.plot(plot_index_final, predictions, label=f'{model_to_plot} Predictions', linestyle='--')\n",
    "        plt.title(f'{model_to_plot}: Actual vs. Predicted Sales (Walk-Forward Validation)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Daily Sales')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Could not plot for {model_to_plot}: Mismatch in length of time index and predictions/actuals.\")\n",
    "        print(f\"  Length of actuals: {len(actuals)}, Total test observations: {total_test_obs}\")\n",
    "else:\n",
    "    print(f\"Model '{model_to_plot}' not found in results for plotting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"  
    }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
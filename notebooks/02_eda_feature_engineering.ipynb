{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_sales_data.csv', parse_dates=['Order Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set 'Order Date' as Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Order Date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales = df['Sales'].resample('D').sum()\n",
    "weekly_sales = df['Sales'].resample('W').sum()\n",
    "monthly_sales = df['Sales'].resample('M').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sales Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "daily_sales.plot(title='Daily Sales Trend')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "weekly_sales.plot(title='Weekly Sales Trend')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "monthly_sales.plot(title='Monthly Sales Trend')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Decomposition (Monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure monthly_sales has enough periods for decomposition (at least 2 full periods)\n",
    "# For monthly data, this means at least 24 months.\n",
    "if len(monthly_sales) >= 24:\n",
    "    decomposition = seasonal_decompose(monthly_sales, model='additive', period=12)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(decomposition.observed, label='Observed')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Observed Monthly Sales')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend, label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Trend Component')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal, label='Seasonal')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Seasonal Component')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid, label='Residual')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Residual Component')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Monthly sales data has less than 24 periods, decomposition might not be meaningful or might fail.\")\n",
    "    # Fallback: Plot monthly sales if decomposition cannot be done\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    monthly_sales.plot(title='Monthly Sales (Decomposition Skipped)')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Time Series Analysis:**\n",
    "- *Daily sales show high volatility.*\n",
    "- *Weekly sales provide a smoother trend, highlighting weekly cyclical patterns.*\n",
    "- *Monthly sales reveal broader trends and seasonality. Sales tend to peak towards the end of the year (November and December) and also show some peaks mid-year (e.g., March, September). There's a general upward trend in sales over the years.*\n",
    "- *The decomposition plot confirms a clear upward trend and a strong yearly seasonality. Residuals appear relatively stable, suggesting the additive model is appropriate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sales Distribution Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Sales by Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Category', 'Sub-Category', 'Region', 'Segment']\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sales_by_col = df.groupby(col)['Sales'].sum().sort_values(ascending=False)\n",
    "    sns.barplot(x=sales_by_col.index, y=sales_by_col.values)\n",
    "    plt.title(f'Total Sales by {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Distribution by Category and Region (Box Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='Category', y='Sales', data=df)\n",
    "plt.title('Sales Distribution by Category')\n",
    "plt.ylabel('Sales')\n",
    "plt.xlabel('Category')\n",
    "plt.ylim(0, df['Sales'].quantile(0.95)) # Limiting y-axis to see distribution better, excluding extreme outliers\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='Region', y='Sales', data=df)\n",
    "plt.title('Sales Distribution by Region')\n",
    "plt.ylabel('Sales')\n",
    "plt.xlabel('Region')\n",
    "plt.ylim(0, df['Sales'].quantile(0.95)) # Limiting y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Sales Distribution Analysis:**\n",
    "- *'Technology' is the highest-grossing category, followed by 'Furniture' and then 'Office Supplies'.*\n",
    "- *Within Sub-Categories, 'Phones' and 'Chairs' are top contributors to sales. 'Fasteners', 'Labels', and 'Art' have the lowest sales.*\n",
    "- *The 'West' region has the highest total sales, followed by 'East', 'Central', and then 'South'.*\n",
    "- *'Consumer' segment leads in sales, followed by 'Corporate' and 'Home Office'.*\n",
    "- *Box plots show that while 'Technology' has higher median sales, it also has a wider spread and more high-value outliers. 'Office Supplies' generally has lower value sales but is very consistent. All regions show a skewed distribution with many lower value sales and some high-value outliers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataFrame's index is 'Order Date' (datetime object)\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "df['Day'] = df.index.day\n",
    "df['DayOfWeek'] = df.index.dayofweek # Monday=0, Sunday=6\n",
    "df['Quarter'] = df.index.quarter\n",
    "df['WeekOfYear'] = df.index.isocalendar().week.astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lag Features for 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to daily sum of sales to ensure one value per day for lags\n",
    "daily_sales_for_lags = df['Sales'].resample('D').sum().fillna(0) # Fill missing days with 0 sales\n",
    "\n",
    "# Create lag features on this daily series\n",
    "df['Sales_Lag_1'] = daily_sales_for_lags.shift(1)\n",
    "df['Sales_Lag_7'] = daily_sales_for_lags.shift(7)\n",
    "df['Sales_Lag_30'] = daily_sales_for_lags.shift(30)\n",
    "\n",
    "# Map these daily lags back to the original DataFrame (which might have multiple entries per day)\n",
    "# This requires aligning the index. We can reindex the original df to include all days from daily_sales_for_lags\n",
    "# then fill forward any new features, and finally select original index.\n",
    "# However, a simpler approach for this context is to merge/join if the original df needs to keep its structure.\n",
    "# For now, let's add these as new columns. This will introduce NaNs for rows where the date doesn't match a daily_sales_for_lags index.\n",
    "# A better way is to create these features on a df that is already daily, then merge back if needed.\n",
    "\n",
    "# Let's ensure df has these columns, filling NaNs that arise from original df not having all dates\n",
    "# This will put the lagged daily sum onto each transaction for that day.\n",
    "df = df.merge(daily_sales_for_lags.shift(1).rename('Sales_Lag_1_Daily'), left_index=True, right_index=True, how='left')\n",
    "df = df.merge(daily_sales_for_lags.shift(7).rename('Sales_Lag_7_Daily'), left_index=True, right_index=True, how='left')\n",
    "df = df.merge(daily_sales_for_lags.shift(30).rename('Sales_Lag_30_Daily'), left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Drop the temporary columns if they were just for calculation\n",
    "if 'Sales_Lag_1' in df.columns: df.drop(columns=['Sales_Lag_1'], inplace=True)\n",
    "if 'Sales_Lag_7' in df.columns: df.drop(columns=['Sales_Lag_7'], inplace=True)\n",
    "if 'Sales_Lag_30' in df.columns: df.drop(columns=['Sales_Lag_30'], inplace=True)\n",
    "\n",
    "df.rename(columns={'Sales_Lag_1_Daily': 'Sales_Lag_1', \n",
    "                   'Sales_Lag_7_Daily': 'Sales_Lag_7', \n",
    "                   'Sales_Lag_30_Daily': 'Sales_Lag_30'}, inplace=True)\n",
    "\n",
    "# Fill NaN values that arise from lags (e.g., first few days)\n",
    "df[['Sales_Lag_1', 'Sales_Lag_7', 'Sales_Lag_30']] = df[['Sales_Lag_1', 'Sales_Lag_7', 'Sales_Lag_30']].fillna(0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Rolling Mean Features for 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the daily_sales_for_lags series (sum of sales per day, 0 for no-sales days)\n",
    "rolling_mean_7d = daily_sales_for_lags.rolling(window=7).mean().fillna(0)\n",
    "rolling_mean_30d = daily_sales_for_lags.rolling(window=30).mean().fillna(0)\n",
    "\n",
    "# Merge these daily rolling means back to the original df\n",
    "df = df.merge(rolling_mean_7d.rename('Sales_Rolling_Mean_7D'), left_index=True, right_index=True, how='left')\n",
    "df = df.merge(rolling_mean_30d.rename('Sales_Rolling_Mean_30D'), left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaNs that might arise if original df has dates not in the daily series (though set_index should align)\n",
    "df[['Sales_Rolling_Mean_7D', 'Sales_Rolling_Mean_30D']] = df[['Sales_Rolling_Mean_7D', 'Sales_Rolling_Mean_30D']].fillna(0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering Summary:**\n",
    "- *Created time-based features: 'Year', 'Month', 'Day', 'DayOfWeek', 'Quarter', 'WeekOfYear'.*\n",
    "- *To create lag and rolling features, daily sales totals were computed first to handle multiple transactions per day and to have a consistent daily signal.*\n",
    "- *Created lag features for sales: 'Sales_Lag_1' (previous day's total sales), 'Sales_Lag_7' (total sales from 7 days ago), 'Sales_Lag_30' (total sales from 30 days ago).*\n",
    "- *Created rolling mean features for sales: 'Sales_Rolling_Mean_7D' (average daily sales over the past 7 days), 'Sales_Rolling_Mean_30D' (average daily sales over the past 30 days).*\n",
    "- *These features will be crucial for time series forecasting models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Storytelling (Summary of Insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performed an Exploratory Data Analysis (EDA) and Feature Engineering on the cleaned superstore sales data.\n",
    "\n",
    "**Key Time Series Insights:**\n",
    "1.  **Sales Trends:** There is a clear upward trend in overall sales over the years. \n",
    "2.  **Seasonality:** Sales exhibit strong yearly seasonality, typically peaking in November and December. Secondary peaks are observed around March and September.\n",
    "3.  **Volatility:** Daily sales are highly volatile. Weekly and monthly aggregations provide a clearer view of underlying trends and seasonality.\n",
    "\n",
    "**Key Sales Distribution Insights:**\n",
    "1.  **Category Performance:** 'Technology' products are the primary revenue drivers, followed by 'Furniture'. 'Office Supplies', while having the lowest sales per transaction, likely represents a high volume of transactions.\n",
    "2.  **Sub-Category Performance:** 'Phones' and 'Chairs' are the top-performing sub-categories. Conversely, items like 'Fasteners', 'Labels', and 'Art' contribute the least to total sales.\n",
    "3.  **Regional Performance:** The 'West' region leads in sales, indicating a strong market presence or higher purchasing power. The 'South' region has the lowest sales among the four.\n",
    "4.  **Customer Segments:** The 'Consumer' segment accounts for the largest portion of sales, followed by 'Corporate' and then 'Home Office'.\n",
    "5.  **Sales Values:** Most transactions are of lower value, with occasional high-value sales, particularly in the 'Technology' category. This indicates a right-skewed sales distribution.\n",
    "\n",
    "**Feature Engineering Undertaken:**\n",
    "- Time-based features (Year, Month, Day, DayOfWeek, Quarter, WeekOfYear) were extracted from the 'Order Date'.\n",
    "- Lag features (1-day, 7-day, 30-day lags of total daily sales) were created to capture past sales influence.\n",
    "- Rolling mean features (7-day and 30-day rolling averages of total daily sales) were generated to smooth out short-term fluctuations and identify local trends.\n",
    "\n",
    "These engineered features enrich the dataset, making it suitable for developing robust time series forecasting models. The insights gained from EDA can also inform business strategies, such_as inventory management based on seasonal demand, targeted marketing for specific regions or customer segments, and product category focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Feature-Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/feature_engineered_sales_data.csv', index=True) # index=True to save 'Order Date'\n",
    "print(\"Feature engineered data saved to '../data/feature_engineered_sales_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
